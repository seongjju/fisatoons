# -*- coding: utf-8 -*-
"""fisatoon_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZAVacvwjB_zUliBEqihEZlNPBVnDuoEl

#### 1️⃣ Google Colab 환경 설정
- GPU 활성화 (런타임 → 런타임 유형 변경 → GPU 선택)
- Google Drive 연동 (학습 데이터 및 모델 저장)
- 필요한 라이브러리 설치 (torch, transformers 등)
"""

# Google Drive 연동
from google.colab import drive
drive.mount('/content/drive')

# # 필수 라이브러리 설치
# # 한글 폰트(Nanum) 설치 (시각화 시 한글 깨짐 방지)
# !apt-get install -y fonts-nanum
# !pip install datasets

"""## 2️⃣ 베이스라인 모델

### Hugging Face의 matthewburke/korean_sentiment 모델 사용
"""

# 테스트용 댓글 데이터 샘플
dic = {0:'NEGATIVE', 1:'POSITIVE'}
X_test = [
        "둘이 합치니까 신체적+정신적 교육 쌉가능이네ㅋㅋㅋㅋㅋ",
        "세라가 원시인인걸 왜 인정하냐 현아 거참 특이한 곳에서 골때리네 ㅋㅋㅋ" ,
        "폭력과 Be폭력을 동시에 경험하는 귀한 인재로군",
        "이러다 언젠간 미리보기 한화당 5개 지르라고 하겠네 ㅋㅋㅋㅋㅋㅋㅋ",
        "솔직히 쳐맞을 애들 많지",
        "어딜가나 늙고 추한것들이 있구만,,,노인공격으로 제대로 보내줬으면",
        "아으으으 이 썩겠다 달달해",
        "와 마지막 진짜 가슴 아프네 저게 어떤 돈인데..."
    ]

y_test = [1, 1, 1, 0, 0, 1, 1, 1]

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# 모델 및 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained("matthewburke/korean_sentiment")
model = AutoModelForSequenceClassification.from_pretrained("matthewburke/korean_sentiment", num_labels=2)

# 파인 튜닝 이전 추론
# 모델을 eval 모델로 전환
model.eval()

# 모델 예측을 담을 preds라는 빈 컨테이너 리스트 생성
preds = []

with torch.no_grad():
  for article in X_test:
    inputs = tokenizer.encode(article, return_tensors="pt",padding=True, truncation=True)
    outputs = model(inputs)
    logits = outputs.logits
    pred = logits.argmax(-1).item()
    preds.append(logits.argmax(-1).item())
    print(f"{dic[pred]}:{article}")

from sklearn.metrics import accuracy_score, classification_report

before_accuracy = accuracy_score(y_test, preds)
before_report = classification_report(y_test, preds, target_names=["NEGATIVE", "POSITIVE"])

print(f"모델 정확도(Accuracy): {before_accuracy:.2f}")
print("\n상세 평가 지표")
print(before_report)

"""## 3️⃣ 파인 튜닝에 쓸 데이터 준비 (영화 리뷰 데이터 활용)
- 네이버 영화 리뷰(NSMC) 데이터셋 전처리
"""

import pandas as pd

# NSMC 네이버 영화 리뷰 감성 데이터 로드 (GitHub에서 직접 불러오기)
df = pd.read_csv("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", sep="\t").dropna()
df = df[['document', 'label']]  # 텍스트와 감성 라벨 (1=긍정, 0=부정)

# 데이터 확인
print(df.head())

# 기본 전처리
import re

# 한글, 숫자, 영어만 남기고 특수문자 제거하는 함수
def clean_text(text):
    text = re.sub(r"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9a-zA-Z ]", "", str(text))
    return text.strip()

# 텍스트 전처리 적용
df['document'] = df['document'].apply(clean_text)

# 데이터 확인
print(df.head())

"""## 4️⃣ 파인 튜닝"""

df.document.values.tolist() # document를 리스트로

# 전체 데이터가 아닌 일부만 사용해도 성능을 충분히 끌어올릴 수 있기 때문에 시간과 자원을 절약 가능
X_train = df.document.tolist()[:100]
y_train = df.label.tolist()[:100]

from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label

# Dataset 생성
dataset = TextDataset(X_train, y_train)

# DataLoader 생성 (batch_size=16)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)

for x, y in dataloader:
  print(x, y)

tokenizer # 토크나이저로 형태소 분리

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

# 모델을 학습 모드로 변경
model.train()

# 에포크 수 지정 및 손실을 담은 빈 컨테이너 리스트 생성
epochs = 30
losses = []

for epoch in range(epochs):

    epoch_loss = 0.0
    batch_count = 0

    for X_batch, y_batch in dataloader:
        # 그래디언트(기울기) 초기화
        optimizer.zero_grad()

        # 문장을 토크나이징하고 인코딩
        inputs = tokenizer.batch_encode_plus(X_batch, return_tensors="pt", padding=True, truncation=True)
        # print(inputs)
        # 모델에 데이터 전달
        outputs = model(**inputs, labels=y_batch)
        # print(outputs)
        # 로짓 추출
        logits = outputs.logits

        # 손실 추출
        loss = outputs.loss
        # 오차역전파
        loss.backward()

        # 가중치(weight) 업데이트
        optimizer.step()

        # 손실을 빈 컨테이너 losses에 순서대로 저장
        epoch_loss += loss.item()
        batch_count += 1

    avg_loss = epoch_loss / batch_count
    # 에포크 및 손실 값 출력
    print(f"epoch:{epoch+1}, loss:{avg_loss}")

    losses.append(avg_loss)

new_losses = [i for i in losses]

import matplotlib.pyplot as plt
plt.plot(new_losses);

# 파인 튜닝 이후 추론
# 모델을 eval 모델로 전환
model.eval()

# 모델 예측을 담을 preds라는 빈 컨테이너 리스트 생성
after_preds = []

# 이하 코드의 설명은 문제 45 코드 참조
with torch.no_grad():
  for article in X_test:
    inputs = tokenizer.encode(article, return_tensors="pt",padding=True, truncation=True)
    outputs = model(inputs)
    logits = outputs.logits
    pred = logits.argmax(-1).item()
    after_preds.append(logits.argmax(-1).item())
    print(f"{dic[pred]}:{article}")

# 파인 튜닝 후 모델 정확도 평가
after_accuracy = accuracy_score(y_test, after_preds)
after_report = classification_report(y_test, after_preds, target_names=["NEGATIVE", "POSITIVE"])

print(f"모델 정확도(Accuracy): {after_accuracy:.2f}")
print("\n상세 평가 지표")
print(after_report)

import matplotlib.pyplot as plt

# 정확도 값과 레이블 리스트 생성
accuracies = [before_accuracy, after_accuracy]
labels = ['Before Fine-tuning', 'After Fine-tuning']

# 그래프 설정 및 출력
plt.figure(figsize=(8,6))
bars = plt.bar(labels, accuracies, color=['skyblue', 'salmon'])
plt.ylim(0, 1)  # 정확도는 0~1 사이의 값
plt.ylabel('Accuracy')
plt.title('Model Accuracy: Before vs After Fine-tuning')

# 각 막대 위에 정확도 값 표시
for bar, acc in zip(bars, accuracies):
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{acc:.2f}',
             ha='center', va='bottom')

plt.show()

##### 모델 저장 경로 설정
model_save_path = "/content/drive/MyDrive/fisatoon/toon_sentiment_model"

tokenizer.save_pretrained(model_save_path)
model.save_pretrained(model_save_path)

import json
import pandas as pd
from transformers import pipeline

# 1. JSON 파일 읽기
json_file_path = "/content/drive/MyDrive/fisatoon/comments.json"  # JSON 파일 경로 (필요에 따라 수정)
with open(json_file_path, "r", encoding="utf-8") as f:
    records = json.load(f)
print("전체 회차 개수:", len(records))

# 2. 파인튜닝된 감성 분석 파이프라인 생성
model_path = "/content/drive/MyDrive/fisatoon/toon_sentiment_model"  # 저장된 파인튜닝 모델 경로
finetuned_classifier = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)

# 3. 결과를 저장할 리스트 초기화
rows = []

# 4. 각 회차별로 댓글 처리
# (각 record에는 webtoon_id, episode, rating, comments 배열이 있음)
# sentiment 결과에서 label은 "LABEL_0"/"LABEL_1" 또는 "NEGATIVE"/"POSITIVE"로 나올 수 있으므로 변환 매핑 설정
label_map = {"LABEL_0": 0, "LABEL_1": 1, "NEGATIVE": 0, "POSITIVE": 1}

for rec in records:
    webtoon_id = rec.get("webtoon_id", "")        # 웹툰 ID
    episode = rec.get("episode", "")              # 화번호
    rating = rec.get("rating", None)              # 실제별점
    comments = rec.get("comments", [])

    # 댓글 텍스트 리스트 추출
    texts = [comment.get("text", "") for comment in comments if "text" in comment]
    if len(texts) == 0:
        continue  # 댓글이 없으면 넘어감

    # 5. 감성 분석 수행 (batch 처리)
    results = finetuned_classifier(texts)

    # 6. 각 댓글에 대해 긍정/부정 값 계산
    pos_values = []
    neg_values = []
    for result in results:
        # result["label"]를 숫자로 변환
        numeric_label = label_map.get(result["label"], None)
        score = result.get("score", 0)
        if numeric_label is None:
            continue
        if numeric_label == 1:
            pos = score         # 긍정일 때 긍정값은 score
            neg = 1 - score     # 부정값은 1-score
        else:
            neg = score         # 부정일 때 부정값은 score
            pos = 1 - score     # 긍정값은 1-score
        pos_values.append(pos)
        neg_values.append(neg)

    # 7. 회차별 평균 긍정비율 및 부정비율 계산 (댓글별 값을 평균)
    if len(pos_values) > 0 and len(neg_values) > 0:
        avg_pos = sum(pos_values) / len(pos_values)
        avg_neg = sum(neg_values) / len(neg_values)
    else:
        avg_pos = avg_neg = None

    # 8. 최종 행 데이터 생성 및 추가
    row = {
        "웹툰": webtoon_id,
        "화번호": episode,
        "긍정비율": avg_pos,
        "부정비율": avg_neg,
        "실제별점": rating
    }
    rows.append(row)

# 9. DataFrame 생성 및 CSV 파일로 저장
comment_df = pd.DataFrame(rows)
csv_filename = "/content/drive/MyDrive/fisatoon/toon_sentiment_model/finetuned_sentiment_results.csv"
comment_df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
print(f"감성 분석 결과 '{csv_filename}'로 저장완료")

# 각 행에 누락이 없는지 확인
len(comment_df)

print(comment_df.isnull().sum())

comment_df.info()